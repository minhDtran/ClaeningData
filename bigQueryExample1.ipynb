{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bigQueryExample1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOS27o6zsIbwEVComtmybMr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minhDtran/ClaeningData/blob/master/bigQueryExample1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uimk8-8Pk_w-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a81fdd8e-e08e-4813-b784-48169bf02c7d"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authenticated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOf8CzVhm1a4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c21ac830-048a-4a51-cdaf-c9333f0a4856"
      },
      "source": [
        "project_id = \"ninth-physics-282816\"\n",
        "from google.cloud import bigquery\n",
        "import humanize\n",
        "client = bigquery.Client(project=project_id)\n",
        "query_job = client.query(\"\"\"\n",
        "    SELECT\n",
        "      CONCAT(\n",
        "        'https://stackoverflow.com/questions/',\n",
        "        CAST(id as STRING)) as url,\n",
        "      view_count\n",
        "    FROM `bigquery-public-data.stackoverflow.posts_questions`\n",
        "    WHERE tags like '%google-bigquery%'\n",
        "    ORDER BY view_count DESC\n",
        "    LIMIT 2\"\"\")\n",
        "\n",
        "results = query_job.result()  # Waits for job to complete.\n",
        "for row in results:\n",
        "    print(\"{} : {} views\".format(row.url, row.view_count))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://stackoverflow.com/questions/22879669 : 65074 views\n",
            "https://stackoverflow.com/questions/35159967 : 63997 views\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue5BGRYpsiMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " PROJECT_ID = \"ninth-physics-282816\"\n",
        " from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import os\n",
        "from six.moves import urllib\n",
        "import tempfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.api_core.exceptions import GoogleAPIError\n",
        "\n",
        "LOCATION = 'us'\n",
        "\n",
        "\n",
        "# Storage directory\n",
        "DATA_DIR = os.path.join(tempfile.gettempdir(), 'census_data')\n",
        "\n",
        "# Download options.\n",
        "DATA_URL = 'https://storage.googleapis.com/cloud-samples-data/ml-engine/census/data'\n",
        "TRAINING_FILE = 'adult.data.csv'\n",
        "EVAL_FILE = 'adult.test.csv'\n",
        "TRAINING_URL = '%s/%s' % (DATA_URL, TRAINING_FILE)\n",
        "EVAL_URL = '%s/%s' % (DATA_URL, EVAL_FILE)\n",
        "\n",
        "DATASET_ID = 'census_dataset'\n",
        "TRAINING_TABLE_ID = 'census_training_table'\n",
        "EVAL_TABLE_ID = 'census_eval_table'\n",
        "\n",
        "CSV_SCHEMA = [\n",
        "      bigquery.SchemaField(\"age\", \"FLOAT64\"),\n",
        "      bigquery.SchemaField(\"workclass\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"fnlwgt\", \"FLOAT64\"),\n",
        "      bigquery.SchemaField(\"education\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"education_num\", \"FLOAT64\"),\n",
        "      bigquery.SchemaField(\"marital_status\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"occupation\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"relationship\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"race\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"gender\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"capital_gain\", \"FLOAT64\"),\n",
        "      bigquery.SchemaField(\"capital_loss\", \"FLOAT64\"),\n",
        "      bigquery.SchemaField(\"hours_per_week\", \"FLOAT64\"),\n",
        "      bigquery.SchemaField(\"native_country\", \"STRING\"),\n",
        "      bigquery.SchemaField(\"income_bracket\", \"STRING\"),\n",
        "  ]\n",
        "\n",
        "UNUSED_COLUMNS = [\"fnlwgt\", \"education_num\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6xiVV1rwG5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_bigquery_dataset_if_necessary(dataset_id):\n",
        "  # Construct a full Dataset object to send to the API.\n",
        "  client = bigquery.Client(project=PROJECT_ID)\n",
        "  dataset = bigquery.Dataset(bigquery.dataset.DatasetReference(PROJECT_ID, dataset_id))\n",
        "  dataset.location = LOCATION\n",
        "\n",
        "  try:\n",
        "    dataset = client.create_dataset(dataset)  # API request\n",
        "    return True\n",
        "  except GoogleAPIError as err:\n",
        "    if err.code != 409: # http_client.CONFLICT\n",
        "      raise\n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnySfJEkwRbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data_into_bigquery(url, table_id):\n",
        "  create_bigquery_dataset_if_necessary(DATASET_ID)\n",
        "  client = bigquery.Client(project=PROJECT_ID)\n",
        "  dataset_ref = client.dataset(DATASET_ID)\n",
        "  table_ref = dataset_ref.table(table_id)\n",
        "  job_config = bigquery.LoadJobConfig()\n",
        "  job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
        "  job_config.source_format = bigquery.SourceFormat.CSV\n",
        "  job_config.schema = CSV_SCHEMA\n",
        "\n",
        "  load_job = client.load_table_from_uri(\n",
        "      url, table_ref, job_config=job_config\n",
        "  )\n",
        "  print(\"Starting job {}\".format(load_job.job_id))\n",
        "\n",
        "  load_job.result()  # Waits for table load to complete.\n",
        "  print(\"Job finished.\")\n",
        "\n",
        "  destination_table = client.get_table(table_ref)\n",
        "  print(\"Loaded {} rows.\".format(destination_table.num_rows))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVvfGftwwZs5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "4a144ad3-a528-4923-da70-240d5756f919"
      },
      "source": [
        "load_data_into_bigquery(TRAINING_URL, TRAINING_TABLE_ID)\n",
        "load_data_into_bigquery(EVAL_URL, EVAL_TABLE_ID)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting job 40fd731f-06b2-4891-9a9c-fe8a7367064f\n",
            "Job finished.\n",
            "Loaded 32561 rows.\n",
            "Starting job f41b6527-84df-4a3c-9052-3fe2ddcab90a\n",
            "Job finished.\n",
            "Loaded 16278 rows.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmvzAGozKRJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "11d1784f-b95f-4988-ef75-a445abb26b2f"
      },
      "source": [
        "\n",
        "!pip install tensorflow.io\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.framework import dtypes\n",
        "from tensorflow_io.bigquery import BigQueryClient\n",
        "from tensorflow_io.bigquery import BigQueryReadSession\n",
        "  \n",
        "def transofrom_row(row_dict):\n",
        "  # Trim all string tensors\n",
        "  trimmed_dict = { column:\n",
        "                  (tf.strings.strip(tensor) if tensor.dtype == 'string' else tensor) \n",
        "                  for (column,tensor) in row_dict.items()\n",
        "                  }\n",
        "  # Extract feature column\n",
        "  income_bracket = trimmed_dict.pop('income_bracket')\n",
        "  # Convert feature column to 0.0/1.0\n",
        "  income_bracket_float = tf.cond(tf.equal(tf.strings.strip(income_bracket), '>50K'), \n",
        "                 lambda: tf.constant(1.0), \n",
        "                 lambda: tf.constant(0.0))\n",
        "  return (trimmed_dict, income_bracket_float)\n",
        "\n",
        "def read_bigquery(table_name):\n",
        "  tensorflow_io_bigquery_client = BigQueryClient()\n",
        "  read_session = tensorflow_io_bigquery_client.read_session(\n",
        "      \"projects/\" + PROJECT_ID,\n",
        "      PROJECT_ID, table_name, DATASET_ID,\n",
        "      list(field.name for field in CSV_SCHEMA \n",
        "           if not field.name in UNUSED_COLUMNS),\n",
        "      list(dtypes.double if field.field_type == 'FLOAT64' \n",
        "           else dtypes.string for field in CSV_SCHEMA\n",
        "           if not field.name in UNUSED_COLUMNS),\n",
        "      requested_streams=2)\n",
        "  \n",
        "  dataset = read_session.parallel_read_rows()\n",
        "  transformed_ds = dataset.map (transofrom_row)\n",
        "  return transformed_ds\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "training_ds = read_bigquery(TRAINING_TABLE_ID).shuffle(10000).batch(BATCH_SIZE)\n",
        "eval_ds = read_bigquery(EVAL_TABLE_ID).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow.io\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/e4/932f64affa1f3bb5d89650fb0af5ff1b93cd8f73671f20096ae6aa191e80/tensorflow_io-0.14.0-cp36-cp36m-manylinux2010_x86_64.whl (21.8MB)\n",
            "\u001b[K     |████████████████████████████████| 21.8MB 55.2MB/s \n",
            "\u001b[?25hCollecting tensorflow<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/be/679ce5254a8c8d07470efb4a4c00345fae91f766e64f1c2aece8796d7218/tensorflow-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2MB 27kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3.0,>=2.2.0->tensorflow.io) (3.2.1)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3.0,>=2.2.0->tensorflow.io) (1.4.1)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/f5/926ae53d6a226ec0fda5208e0e581cffed895ccc89e36ba76a8e60895b78/tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 27.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3.0,>=2.2.0->tensorflow.io) (1.1.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3.0,>=2.2.0->tensorflow.io) (2.10.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3.0,>=2.2.0->tensorflow.io) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3.0,>=2.2.0->tensorflow.io) (1.18.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3.0,>=2.2.0->tensorflow.io) (0.2.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3.0,>=2.2.0->tensorflow.io) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3.0,>=2.2.0->tensorflow.io) (1.30.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3.0,>=2.2.0->tensorflow.io) (1.12.1)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/74/0a6fcb206dcc72a6da9a62dd81784bfdbff5fedb099982861dc2219014fb/tensorboard-2.2.2-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 42.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3.0,>=2.2.0->tensorflow.io) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3.0,>=2.2.0->tensorflow.io) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3.0,>=2.2.0->tensorflow.io) (0.9.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.3.0,>=2.2.0->tensorflow.io) (0.34.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow<2.3.0,>=2.2.0->tensorflow.io) (47.3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3.0,>=2.2.0->tensorflow.io) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3.0,>=2.2.0->tensorflow.io) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3.0,>=2.2.0->tensorflow.io) (1.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3.0,>=2.2.0->tensorflow.io) (3.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3.0,>=2.2.0->tensorflow.io) (0.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow<2.3.0,>=2.2.0->tensorflow.io) (1.6.0.post3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3.0,>=2.2.0->tensorflow.io) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3.0,>=2.2.0->tensorflow.io) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3.0,>=2.2.0->tensorflow.io) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3.0,>=2.2.0->tensorflow.io) (2020.6.20)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3.0,>=2.2.0->tensorflow.io) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3.0,>=2.2.0->tensorflow.io) (4.1.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3.0,>=2.2.0->tensorflow.io) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3.0,>=2.2.0->tensorflow.io) (1.6.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3.0,>=2.2.0->tensorflow.io) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3.0,>=2.2.0->tensorflow.io) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3.0,>=2.2.0->tensorflow.io) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow<2.3.0,>=2.2.0->tensorflow.io) (3.1.0)\n",
            "Installing collected packages: tensorflow-estimator, gast, tensorboard, tensorflow, tensorflow.io\n",
            "  Found existing installation: tensorflow-estimator 2.0.1\n",
            "    Uninstalling tensorflow-estimator-2.0.1:\n",
            "      Successfully uninstalled tensorflow-estimator-2.0.1\n",
            "  Found existing installation: gast 0.2.2\n",
            "    Uninstalling gast-0.2.2:\n",
            "      Successfully uninstalled gast-0.2.2\n",
            "  Found existing installation: tensorboard 2.0.2\n",
            "    Uninstalling tensorboard-2.0.2:\n",
            "      Successfully uninstalled tensorboard-2.0.2\n",
            "  Found existing installation: tensorflow 2.0.0\n",
            "    Uninstalling tensorflow-2.0.0:\n",
            "      Successfully uninstalled tensorflow-2.0.0\n",
            "Successfully installed gast-0.3.3 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0 tensorflow.io\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRaYCbgly2_2",
        "colab_type": "text"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7fhQycLPRk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import feature_column\n",
        "\n",
        "feature_columns = []\n",
        "\n",
        "# numeric cols\n",
        "for header in ['capital_gain', 'capital_loss', 'hours_per_week']:\n",
        "  feature_columns.append(feature_column.numeric_column(header))\n",
        "\n",
        "# categorical cols\n",
        "for header in ['workclass', 'marital_status', 'occupation', 'relationship',\n",
        "               'race', 'native_country', 'education']:\n",
        "  categorical_feature = feature_column.categorical_column_with_vocabulary_list(\n",
        "        header, get_categorical_feature_values(header))\n",
        "  categorical_feature_one_hot = feature_column.indicator_column(categorical_feature)\n",
        "  feature_columns.append(categorical_feature_one_hot)\n",
        "\n",
        "# bucketized cols\n",
        "age = feature_column.numeric_column('age')\n",
        "age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
        "feature_columns.append(age_buckets)\n",
        "\n",
        "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UELQZK-QPbdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Dense = tf.keras.layers.Dense\n",
        "model = tf.keras.Sequential(\n",
        "  [\n",
        "    feature_layer,\n",
        "      Dense(100, activation=tf.nn.relu, kernel_initializer='uniform'),\n",
        "      Dense(75, activation=tf.nn.relu),\n",
        "      Dense(50, activation=tf.nn.relu),\n",
        "      Dense(25, activation=tf.nn.relu),\n",
        "      Dense(1, activation=tf.nn.sigmoid)\n",
        "  ])\n",
        "\n",
        "# Compile Keras model\n",
        "model.compile(\n",
        "    loss='binary_crossentropy', \n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXAXPdZ-Pi14",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "5b65c70d-cacc-4941-fca6-6622b78326ed"
      },
      "source": [
        "model.fit(training_ds, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "WARNING:tensorflow:Layer dense_features is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "1018/1018 [==============================] - 5s 5ms/step - loss: 0.6213 - accuracy: 0.8097\n",
            "Epoch 2/5\n",
            "1018/1018 [==============================] - 5s 5ms/step - loss: 0.3572 - accuracy: 0.8348\n",
            "Epoch 3/5\n",
            "1018/1018 [==============================] - 5s 5ms/step - loss: 0.3506 - accuracy: 0.8388\n",
            "Epoch 4/5\n",
            "1018/1018 [==============================] - 5s 5ms/step - loss: 0.3428 - accuracy: 0.8436\n",
            "Epoch 5/5\n",
            "1018/1018 [==============================] - 5s 5ms/step - loss: 0.3409 - accuracy: 0.8455\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f52fbd1d5f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j63oW07VPxrp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3368d0ab-097c-4266-90fb-3c1d279c0681"
      },
      "source": [
        "loss, accuracy = model.evaluate(eval_ds)\n",
        "print(\"Accuracy\", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "509/509 [==============================] - 2s 4ms/step - loss: 0.3648 - accuracy: 0.8376\n",
            "Accuracy 0.8375721573829651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb14bbubP3hA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f5307b58-0247-4acd-ec70-f38ab4988cf5"
      },
      "source": [
        "sample_x = {\n",
        "    'age' : np.array([56, 36]), \n",
        "    'workclass': np.array(['Local-gov', 'Private']), \n",
        "    'education': np.array(['Bachelors', 'Bachelors']), \n",
        "    'marital_status': np.array(['Married-civ-spouse', 'Married-civ-spouse']), \n",
        "    'occupation': np.array(['Tech-support', 'Other-service']), \n",
        "    'relationship': np.array(['Husband', 'Husband']), \n",
        "    'race': np.array(['White', 'Black']), \n",
        "    'gender': np.array(['Male', 'Male']), \n",
        "    'capital_gain': np.array([0, 7298]), \n",
        "    'capital_loss': np.array([0, 0]), \n",
        "    'hours_per_week': np.array([40, 36]), \n",
        "    'native_country': np.array(['United-States', 'United-States'])\n",
        "  }\n",
        "\n",
        "model.predict(sample_x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.74135274],\n",
              "       [0.8467775 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C50_vCOy73A",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}